Directory structure:
└── dmboynton56-llm-advisor/
    ├── README.md
    ├── main.py
    ├── requirements.txt
    ├── config/
    │   ├── init.py
    │   └── settings.py
    ├── data/
    │   └── daily_news
    ├── docs/
    │   ├── EXECUTION.md
    │   ├── FILE_STRUCTURE.md
    │   ├── PROJECT_PLAN.md
    │   └── STRATEGY.md
    ├── models/
    │   ├── AAPL_label_encoder.pkl
    │   ├── AMZN_label_encoder.pkl
    │   ├── GOOG_label_encoder.pkl
    │   ├── IWM_label_encoder.pkl
    │   ├── META_label_encoder.pkl
    │   ├── MSFT_label_encoder.pkl
    │   ├── NVDA_label_encoder.pkl
    │   ├── QQQ_label_encoder.pkl
    │   ├── SPY_label_encoder.pkl
    │   └── TSLA_label_encoder.pkl
    ├── prompts/
    │   └── prompts.json
    ├── reports/
    │   ├── AAPL_rf_feature_importances.csv
    │   ├── AMZN_rf_feature_importances.csv
    │   ├── GOOG_rf_feature_importances.csv
    │   ├── IWM_rf_feature_importances.csv
    │   ├── META_rf_feature_importances.csv
    │   ├── MSFT_rf_feature_importances.csv
    │   ├── NVDA_rf_feature_importances.csv
    │   ├── QQQ_rf_feature_importances.csv
    │   ├── SPY_rf_feature_importances.csv
    │   └── TSLA_rf_feature_importances.csv
    ├── src/
    │   ├── init.py
    │   └── data_processing/
    │       └── news_scraper.py
    └── training/
        ├── train_daily_bias_models.py
        └── notebooks/
            └── feature_engineering.ipynb

================================================
FILE: README.md
================================================
# llm-advisor

# Liquidity Flow Agent

**An AI-powered day trading bot that combines traditional machine learning with Large Language Models (LLMs) to execute a sophisticated ICT-based trading strategy.**

This project leverages a hybrid approach:
1.  **Daily Bias Prediction:** Pre-trained ML models (Random Forest/XGBoost) analyze historical data to determine the most probable market direction for the day (Bullish/Bearish/Choppy) for a watchlist of symbols.
2.  **LLM-Powered Real-Time Analysis:** An AI agent, powered by models like GPT-4o, continuously analyzes real-time market data, looking for specific ICT entry patterns that align with the pre-determined daily bias.
3.  **Automated Execution:** When the AI agent identifies a high-confidence trade setup, it autonomously calculates risk parameters and executes the trade via the Alpaca API.

---

## 🚀 Quick Start

1.  **Clone the repository:**
    ```bash
    git clone [repository-url]
    cd liquidity_flow_agent
    ```

2.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Configure your settings:**
    * Create a `.env` file and add your `ALPACA_` and `OPENAI_` API keys.
    * Edit `config/settings.py` to define your symbol watchlist and risk parameters.

4.  **Train Daily Bias Models (if not already present):**
    ```bash
    python training/train_daily_bias_models.py
    ```

5.  **Run the bot:**
    ```bash
    python main.py
    ```

---

## 📚 Project Documentation

For a complete understanding of the system, please refer to the documentation:

* **[Project Plan](./docs/PROJECT_PLAN.md):** The overall architecture, data flow, and development roadmap.
* **[Trading Strategy](./docs/STRATEGY.md):** A detailed explanation of the 5-step ICT methodology used by the AI agent.
* **[Execution & Risk](./docs/EXECUTION.md):** The rules governing trade execution, position sizing, and risk management.


================================================
FILE: main.py
================================================
import os
import sys
import json
import time
from datetime import datetime
import pytz

# Add project root to path to allow for imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Import the core modules of our bot
from src.data_processing.news_scraper import fetch_news_headlines
from src.strategy.bias_calculator import calculate_all_biases
from src.api_clients.llm_client import get_ai_analysis # Assumes you build this
from config.settings import WATCHLIST, CONFIDENCE_THRESHOLD

# --- Placeholder imports for modules to be built ---
# from src.data_processing.price_aggregator import get_realtime_bars
# from src.execution.order_manager import execute_trade

# A helper function to load prompts from the JSON file
def load_prompt(prompt_name):
    """Loads a specific prompt's content from the prompts.json file."""
    try:
        prompts_path = os.path.join(os.path.dirname(__file__), 'prompts', 'prompts.json')
        with open(prompts_path, 'r') as f:
            all_prompts = json.load(f)
        return all_prompts[prompt_name]['content']
    except (FileNotFoundError, KeyError) as e:
        print(f"Error loading prompt '{prompt_name}': {e}")
        return None

def run_pre_market_analysis():
    """
    Runs all pre-market tasks and uses an LLM to synthesize them into a final context object.
    """
    print("="*60)
    print("Phase 1: Running Pre-Market Analysis...")
    print(f"Timestamp: {datetime.now(pytz.timezone('US/Eastern')).strftime('%Y-%m-%d %H:%M:%S')} EST")
    print("="*60)
    
    # === Step 1: Gather Raw Data ===
    print("  Fetching raw data points...")
    raw_headlines = fetch_news_headlines()
    headlines_text = "\n".join(f"- {h}" for h in raw_headlines)
    
    raw_ml_biases = calculate_all_biases(WATCHLIST)
    ml_biases_json_str = json.dumps(raw_ml_biases, indent=2)

    # === Step 2: Synthesize Context with an LLM ===
    print("  Sending raw data to LLM for synthesis into a 'Daily Briefing'...")
    
    synthesis_prompt_template = load_prompt("pre_market_synthesis_prompt")
    if not synthesis_prompt_template:
        return None

    synthesis_prompt = synthesis_prompt_template.replace("{{ml_bias_data}}", ml_biases_json_str)
    synthesis_prompt = synthesis_prompt.replace("{{news_headlines}}", headlines_text)

    # Call the LLM to get the final, synthesized context object
    final_daily_context = get_ai_analysis(
        system_prompt="You are a Senior Trading Analyst that provides structured JSON output.",
        user_prompt=synthesis_prompt,
        json_response=True # Assume your client can parse a JSON string from the LLM
    )

    if not final_daily_context:
        print("  FATAL: Could not generate synthesized daily context from LLM. Exiting.")
        return None

    print("\n--- Pre-Market 'Daily Briefing' Assembled & Synthesized ---")
    print(json.dumps(final_daily_context, indent=2))
    print("="*60)
    
    return final_daily_context

def main_trading_loop(daily_context):
    """
    The main loop that runs continuously during trading hours.
    """
    print("\nPhase 2: Entering Main Trading Loop...")
    
    while True: # In a real bot, you'd add logic to check market hours and stop
        print(f"\n--- New Analysis Cycle: {datetime.now(pytz.timezone('US/Eastern')).strftime('%H:%M:%S')} EST ---")
        
        # 1. Get real-time price data for all symbols (Placeholder)
        # realtime_market_data = get_realtime_bars(WATCHLIST)
        print("  (Placeholder) Fetched real-time price data...")
        realtime_market_data = {} # This would be a populated dictionary
        
        # 2. Send synthesized context and real-time data to LLM for analysis (Placeholder)
        # analysis_result = analyze_for_trades(daily_context, realtime_market_data)
        print("  (Placeholder) Sent data to LLM, querying for trade signals...")
        analysis_result = {} # This would be the parsed JSON response from the LLM

        # 3. Decision Gate: Check the results for a high-confidence trade
        if analysis_result and "trade_analysis" in analysis_result:
            for trade in analysis_result["trade_analysis"]:
                if trade.get("setup_found") and trade.get("confidence_score", 0) >= CONFIDENCE_THRESHOLD:
                    print(f"  🚨 HIGH-CONFIDENCE SIGNAL FOUND FOR {trade['symbol']}! 🚨")
                    print(f"     Confidence: {trade['confidence_score']}%")
                    print(f"     Reasoning: {trade['reasoning']}")
                    
                    # 4. Execute the trade
                    # execute_trade(trade['trade_parameters'])
                    print(f"     ---> (SIMULATED) EXECUTING TRADE: {trade['trade_parameters']}")
                    
                    print("     Pausing for 5 minutes after trade execution...")
                    time.sleep(300) 
                    break 
            else:
                 print("  No high-confidence trade setups found in this cycle.")
        
        # Wait for the next cycle
        time.sleep(60)

if __name__ == "__main__":
    # Run the pre-market analysis once at the start
    context = run_pre_market_analysis()
    
    if context:
        print("\nPre-market analysis complete. The bot would now enter the main trading loop.")
        # To run the bot continuously, uncomment the line below:
        # main_trading_loop(context)


================================================
FILE: requirements.txt
================================================
alpaca-py
openai
python-dotenv
pandas
numpy
scikit-learn
xgboost
yfinance
beautifulsoup4
requests
notebook
matplotlib
seaborn


================================================
FILE: config/init.py
================================================
[Empty file]


================================================
FILE: config/settings.py
================================================
# ==============================================================================
# Liquidity Flow Agent - Configuration File
# ==============================================================================

# --- API Keys -----------------------------------------------------------------
# These should be stored in your .env file, not here. This file will read them.
# Example .env file:
# ALPACA_API_KEY="PK..."
# ALPACA_SECRET_KEY="sk..."
# ALPACA_PAPER_TRADING="true"
# OPENAI_API_KEY="sk-..."
# ANTHROPIC_API_KEY="sk-..."

# --- Watchlist ----------------------------------------------------------------
# List of symbols the bot will actively monitor.
WATCHLIST = [
    "SPY", "QQQ", "IWM",  # Indices & Futures
    "NVDA", "TSLA", "AAPL", "AMZN",       # Tech Stocks
    "META", "MSFT", "GOOG"                # More Tech Stocks
]

# --- Trading Parameters -------------------------------------------------------
# The bot will only trade one position at a time.
MAX_CONCURRENT_TRADES = 1

# The window during which the bot is allowed to open new trades.
# Format: "HH:MM" in US/Eastern timezone.
TRADING_WINDOW_START = "09:30"
TRADING_WINDOW_END = "12:00"

# The time to close all open positions, regardless of P/L.
END_OF_DAY_CLOSE_TIME = "15:50"

# --- Risk Management ----------------------------------------------------------
# The maximum percentage of total account equity to risk on a single trade.
# Example: 1.0 means 1% risk.
MAX_RISK_PER_TRADE_PERCENT = 50.0

# The minimum required risk-to-reward ratio for a trade to be considered valid.
# Example: 2.5 means the potential profit must be at least 2.5 times the potential loss.
MINIMUM_RISK_REWARD_RATIO = 2.0

# --- Strategy & AI Parameters -------------------------------------------------
# The confidence score threshold required to trigger a trade.
# This is the aggregated score from the LLM analysis.
CONFIDENCE_THRESHOLD = 85  # Integer from 0 to 100

# The LLM provider(s) to use for analysis. Can be "openai", "anthropic", or a list.
LLM_PROVIDERS = ["openai"]

# The specific model to use for the main analysis.
LLM_MODEL = "gpt-4o"

# The time in seconds the bot will wait between each analysis loop.
ANALYSIS_INTERVAL_SECONDS = 30

# --- Logging Configuration ----------------------------------------------------
LOG_LEVEL = "INFO"  # "DEBUG", "INFO", "WARNING", "ERROR"
LOG_FILE = "logs/trading_agent.log"



================================================
FILE: data/daily_news
================================================
[Empty file]


================================================
FILE: docs/EXECUTION.md
================================================
# Trading Strategy: ICT Liquidity Flow Model

This document defines the 5-step, top-down trading methodology that the AI agent (`LiquidityFlowGPT`) must follow. The strategy is based on Inner Circle Trader (ICT) concepts, focusing on how institutional order flow creates predictable patterns of liquidity.

## The 5-Step Methodology

### Step 1: Determine Daily Bias
**Objective:** Identify the most likely direction for the day's primary price move.
* **Method:** Analyze High-Timeframe (HTF) charts (1-hour, 4-hour, Daily) to identify key **Draws on Liquidity**. These are price levels where a significant number of buy-stops or sell-stops are likely resting.
* **Key Levels to Mark:**
    * Previous Day High/Low
    * Previous Session High/Low (Asia, London)
    * Old Highs/Lows on the 1h/4h chart
    * Highs/Lows created by major news or volume events.
* **Thesis:** The market will likely move toward the most obvious pool of liquidity. A bias is **Bullish** if the draw is to the upside; **Bearish** if the draw is to the downside.

### Step 2: Await Reversal at Key Levels
**Objective:** Wait for a "stop hunt" or liquidity grab at a key HTF level.
* **Method:** Once the daily bias is established, patiently wait for the price to sweep a key level *against* the expected direction (e.g., a sweep of the previous day's low in a Bullish bias). This is often a manipulation move to fill institutional orders.
* **Action:** Once the sweep occurs, zoom into the 5-minute chart to watch for the first sign of a reversal.

### Step 3: Confirm Low-Timeframe (LTF) Reversal
**Objective:** Confirm that the manipulation is over and the true trend is resuming.
* **Method:** Look for a **confluence** of at least two confirmation signals on the 5-minute chart.
* **Confirmation Signals:**
    * **Break of Structure (BOS):** A decisive close above a recent swing high (for bullish) or below a swing low (for bearish). This is the most important signal.
    * **Inverse Fair Value Gap (iFVG):** A small FVG that forms *against* the new direction and is immediately disrespected and traded through.
    * **79% Fibonacci Retracement:** A bounce from the 79% ("Optimal Trade Entry") level of the initial reversal move.
    * **SMT Divergence:** A divergence between correlated assets (e.g., ES/NQ or SPY/QQQ) where one fails to make a new high/low while the other succeeds.

### Step 4: Identify LTF Entry
**Objective:** Find a precise, low-risk entry point to join the newly confirmed trend.
* **Method:** After the 5-minute reversal is confirmed, wait for a pullback to a high-probability continuation setup. Then, refine the entry on the 1-minute chart.
* **5-Minute Continuation Setups:**
    * **Fair Value Gap (FVG):** A 3-candle imbalance left during the reversal move. This is the highest probability entry.
    * **Order Block:** The last down-candle before a strong up-move (or vice-versa).
    * **Breaker Block:** A failed order block that price has traded through.
* **1-Minute Refinement:** Look for a 1-minute BOS or iFVG *within* the 5-minute FVG to time the entry with maximum precision.

### Step 5: Define Trade Parameters
**Objective:** Set clear exit points based on market structure.
* **Entry:** Taken at the 1-minute confirmation within the 5-minute continuation setup.
* **Stop Loss:** Placed just below the swing low of the reversal structure (for bullish) or above the swing high (for bearish).
* **Take Profit:** Set at the next major HTF Draw on Liquidity, as identified in Step 1.


================================================
FILE: docs/FILE_STRUCTURE.md
================================================
[Binary file]


================================================
FILE: docs/PROJECT_PLAN.md
================================================
# Project Plan: Liquidity Flow Agent

This document outlines the architecture, data flow, and development plan for the AI-powered trading bot.

## 1. System Architecture

The system is designed as a modular, event-driven application that operates in distinct phases throughout the trading day.

**Key Components:**
* **Data Processing:** Gathers, cleans, and formats all necessary data (market prices, news).
* **Strategy & Analysis:** Contains the core logic for both the ML-based bias prediction and the LLM-based real-time analysis.
* **API Clients:** Manages all external API communications (Alpaca for trading, LLM providers for analysis).
* **Execution:** Handles the placement and management of trades.

## 2. Data & Logic Flow

The bot's operational flow is a top-down process, moving from a high-level daily view to a micro-level 1-minute analysis.

**Phase 1: Pre-Market (approx. 9:00 AM EST)**
1.  **News Scraping:** The `news_scraper.py` module gathers key financial headlines and articles for the monitored symbols.
2.  **Daily Bias Calculation:** The `bias_calculator.py` module loads the pre-trained `.pkl` models. It uses the latest market data to calculate the daily bias (Bullish/Bearish/Choppy) and a confidence score for each symbol.
3.  **Context Compilation:** The daily biases and summarized news are compiled as the foundational context for the day.

**Phase 2: Market Open & Monitoring (9:30 AM EST onwards)**
1.  **Real-Time Data Feed:** The `price_aggregator.py` begins polling the Alpaca API every 30-60 seconds for the latest price data across the watchlist.
2.  **Feature Calculation:** The `feature_engine.py` calculates key technical metrics from the real-time data.
3.  **LLM Analysis Loop:**
    * The `trade_analyzer.py` formats the daily context (bias, news) and the latest real-time data into a structured prompt.
    * This prompt is sent to one or more LLM APIs via the `llm_client.py`.
    * The bot asks the LLM to analyze the data according to the ICT strategy and return a verdict, including a confidence score.
4.  **Decision Gate:** The main loop in `main.py` aggregates the confidence scores from the LLMs.

**Phase 3: Trade Execution**
1.  **Trigger:** If the combined confidence score for a specific symbol's setup crosses a pre-defined threshold (e.g., 85%), a trade signal is generated.
2.  **Parameter Calculation:** The LLM is asked for precise stop loss and take profit levels.
3.  **Order Placement:** The `order_manager.py` receives the final trade parameters and places the bracket order via the `alpaca_client.py`.

## 3. Development Plan

The project will be built in phases to ensure each component is robust before integration.

* **Phase 1: Foundation & Data:** Build the Alpaca client, data aggregators, and feature engine. Ensure a reliable stream of real-time data.
* **Phase 2: Daily Bias Integration:** Integrate the pre-trained models from `ICTML` to ensure the bot can generate daily biases at the start of each session.
* **Phase 3: LLM Agent Core:** Develop the `llm_client` and `trade_analyzer`. Focus on prompt engineering and parsing structured JSON responses.
* **Phase 4: Execution & Paper Trading:** Build the `order_manager` and deploy the full system in Alpaca's paper trading environment for at least 1-2 months.
* **Phase 5: Live Deployment:** After consistent profitability in paper trading, deploy with a small amount of real capital and scale gradually.


================================================
FILE: docs/STRATEGY.md
================================================
# Trading Strategy: ICT Liquidity Flow Model

This document defines the 5-step, top-down trading methodology that the AI agent (`LiquidityFlowGPT`) must follow. The strategy is based on Inner Circle Trader (ICT) concepts, focusing on how institutional order flow creates predictable patterns of liquidity.

## The 5-Step Methodology

### Step 1: Determine Daily Bias
**Objective:** Identify the most likely direction for the day's primary price move.
* **Method:** Analyze High-Timeframe (HTF) charts (1-hour, 4-hour, Daily) to identify key **Draws on Liquidity**. These are price levels where a significant number of buy-stops or sell-stops are likely resting.
* **Key Levels to Mark:**
    * Previous Day High/Low
    * Previous Session High/Low (Asia, London)
    * Old Highs/Lows on the 1h/4h chart
    * Highs/Lows created by major news or volume events.
* **Thesis:** The market will likely move toward the most obvious pool of liquidity. A bias is **Bullish** if the draw is to the upside; **Bearish** if the draw is to the downside.

### Step 2: Await Reversal at Key Levels
**Objective:** Wait for a "stop hunt" or liquidity grab at a key HTF level.
* **Method:** Once the daily bias is established, patiently wait for the price to sweep a key level *against* the expected direction (e.g., a sweep of the previous day's low in a Bullish bias). This is often a manipulation move to fill institutional orders.
* **Action:** Once the sweep occurs, zoom into the 5-minute chart to watch for the first sign of a reversal.

### Step 3: Confirm Low-Timeframe (LTF) Reversal
**Objective:** Confirm that the manipulation is over and the true trend is resuming.
* **Method:** Look for a **confluence** of at least two confirmation signals on the 5-minute chart.
* **Confirmation Signals:**
    * **Break of Structure (BOS):** A decisive close above a recent swing high (for bullish) or below a swing low (for bearish). This is the most important signal.
    * **Inverse Fair Value Gap (iFVG):** A small FVG that forms *against* the new direction and is immediately disrespected and traded through.
    * **79% Fibonacci Retracement:** A bounce from the 79% ("Optimal Trade Entry") level of the initial reversal move.
    * **SMT Divergence:** A divergence between correlated assets (e.g., ES/NQ or SPY/QQQ) where one fails to make a new high/low while the other succeeds.

### Step 4: Identify LTF Entry
**Objective:** Find a precise, low-risk entry point to join the newly confirmed trend.
* **Method:** After the 5-minute reversal is confirmed, wait for a pullback to a high-probability continuation setup. Then, refine the entry on the 1-minute chart.
* **5-Minute Continuation Setups:**
    * **Fair Value Gap (FVG):** A 3-candle imbalance left during the reversal move. This is the highest probability entry.
    * **Order Block:** The last down-candle before a strong up-move (or vice-versa).
    * **Breaker Block:** A failed order block that price has traded through.
* **1-Minute Refinement:** Look for a 1-minute BOS or iFVG *within* the 5-minute FVG to time the entry with maximum precision.

### Step 5: Define Trade Parameters
**Objective:** Set clear exit points based on market structure.
* **Entry:** Taken at the 1-minute confirmation within the 5-minute continuation setup.
* **Stop Loss:** Placed just below the swing low of the reversal structure (for bullish) or above the swing high (for bearish).
* **Take Profit:** Set at the next major HTF Draw on Liquidity, as identified in Step 1.


================================================
FILE: models/AAPL_label_encoder.pkl
================================================
[Binary file]


================================================
FILE: models/AMZN_label_encoder.pkl
================================================
[Binary file]


================================================
FILE: models/GOOG_label_encoder.pkl
================================================
[Binary file]


================================================
FILE: models/IWM_label_encoder.pkl
================================================
[Binary file]


================================================
FILE: models/META_label_encoder.pkl
================================================
[Binary file]


================================================
FILE: models/MSFT_label_encoder.pkl
================================================
[Binary file]


================================================
FILE: models/NVDA_label_encoder.pkl
================================================
[Binary file]


================================================
FILE: models/QQQ_label_encoder.pkl
================================================
[Binary file]


================================================
FILE: models/SPY_label_encoder.pkl
================================================
[Binary file]


================================================
FILE: models/TSLA_label_encoder.pkl
================================================
[Binary file]


================================================
FILE: prompts/prompts.json
================================================
{
    "system_prompt": {
      "role": "system",
      "content": "You are 'LiquidityFlowGPT', an AI trading analyst specializing in ICT (Inner Circle Trader) concepts. Your sole purpose is to provide high-probability trade ideas by executing a precise, top-down market analysis. You must adhere strictly to the following 5-step methodology:\n\n1.  **Determine Daily Bias:** Identify the daily bias (Bullish/Bearish) by analyzing High-Timeframe (HTF) draws on liquidity.\n2.  **Await Reversal at Key Levels:** Wait for an HTF key level to be swept, then monitor the 5-minute (LTF) chart for a change in market structure.\n3.  **Confirm LTF Reversal:** Identify a high-probability reversal on the 5-minute chart using a confluence of confirmation signals (BOS, iFVG, 79% Fib, SMT Divergence).\n4.  **Identify LTF Entry:** After confirming the reversal, identify a 5-minute continuation setup (FVG, Order Block, Breaker Block) for entry, refined on the 1-minute chart.\n5.  **Define Trade Parameters:** Define a precise entry, stop loss, and take profit based on HTF liquidity and new intraday structure.\n\nYour analysis must be objective, data-driven, and strictly follow this sequence. You must respond ONLY in JSON format."
    },
    "news_summary_prompt": {
      "role": "user",
      "content": "You are a financial news analyst. Your task is to read the following headlines and article snippets and synthesize them into a single, concise paragraph (max 3 sentences) that summarizes the overall market sentiment for the upcoming trading session. Focus on macroeconomic news and sentiment for the major indices (S&P 500, Nasdaq).\n\n**News Articles:**\n**Headline:** {{headline_1}}\n**Snippet:** {{snippet_1}}\n---\n**Headline:** {{headline_2}}\n**Snippet:** {{snippet_2}}\n\n**Your Summary:**"
    },
    "pre_market_synthesis_prompt": {
      "role": "user",
      "content": "## Pre-Market Analysis Synthesis Request\n\n**Task:** You are a Senior Trading Analyst. Your task is to synthesize the following raw data points into a single, structured JSON object that will serve as the daily briefing for a trading agent. Your analysis should be concise and actionable.\n\n**1. Raw ML Model Daily Bias Predictions:**\n```json\n{{ml_bias_data}}\n```\n\n**2. Raw News Headlines:**\n```text\n{{news_headlines}}\n```\n\n**Your Response:**\nAnalyze both the quantitative model outputs and the qualitative news headlines. Fuse them into a single JSON object with the following structure:\n- `analysis_timestamp`: The current ISO 8601 timestamp.\n- `news_sentiment_summary`: A 1-2 sentence summary of the overall market sentiment based on the news.\n- `key_market_catalysts`: A list of the top 3-4 news events or themes driving the market today.\n- `synthesized_daily_biases`: A list of objects, one for each symbol. For each symbol, provide the ML model's bias and confidence, your `final_bias` (which can be 'Bullish', 'Bearish', 'Choppy', or 'Cautiously Bullish/Bearish'), and a brief `reasoning` explaining how the news confirms, contradicts, or adds nuance to the ML model's prediction."
    },
    "main_analysis_prompt": {
      "role": "user",
      "content": "## Real-Time Analysis Request\n\n**Task:** Analyze the following market data according to your 5-step methodology and provide a verdict.\n\n**Daily Context (Synthesized Briefing):**\n```json\n{{daily_context}}\n```\n\n**Real-Time Price Data (Latest 15 minutes):**\n```json\n{{realtime_price_data}}\n```\n\n**Your Response:**\nAnalyze each symbol on the watchlist and provide your analysis in the specified JSON format."
    }
  }


================================================
FILE: reports/AAPL_rf_feature_importances.csv
================================================
,0
close_vs_range,0.29069298830942136
body_pct,0.2672324422309135
lower_wick_pct,0.08067789579973884
upper_wick_pct,0.07833684755407481
current_range_pct,0.07624111121128611
swept_prev_low,0.06184144906831547
h4_close_vs_sma20,0.022588677885499692
gap_pct,0.022208024515590667
prev_day_range_pct,0.02211977040373652
gap_size_abs,0.021362377715337194
swept_prev_high,0.020995106934623226
h1_eod_volume_ratio,0.01851214492628362
volume_ratio_5d,0.017191163445178965



================================================
FILE: reports/AMZN_rf_feature_importances.csv
================================================
,0
close_vs_range,0.3115995919902732
body_pct,0.25726744769664295
current_range_pct,0.08722345499323834
upper_wick_pct,0.0736964989450058
lower_wick_pct,0.07219058457612328
volume_ratio_5d,0.037323082396512476
gap_size_abs,0.027842155522262546
swept_prev_low,0.027260699822347467
swept_prev_high,0.026864270022606815
gap_pct,0.024610596716638393
h4_close_vs_sma20,0.018743088235870113
h1_eod_volume_ratio,0.01792223288243958
prev_day_range_pct,0.017456296200039106



================================================
FILE: reports/GOOG_rf_feature_importances.csv
================================================
,0
body_pct,0.2895381998384731
close_vs_range,0.2844961105112853
lower_wick_pct,0.0987531194591872
current_range_pct,0.06639502635524207
upper_wick_pct,0.06570967177402717
swept_prev_low,0.03748577250426549
volume_ratio_5d,0.03189343266991812
swept_prev_high,0.02678738096031214
h4_close_vs_sma20,0.0226884839945873
h1_eod_volume_ratio,0.022074942149685303
gap_pct,0.02045579976014387
prev_day_range_pct,0.019041585705943737
gap_size_abs,0.014680474316929107



================================================
FILE: reports/IWM_rf_feature_importances.csv
================================================
,0
close_vs_range,0.32539512273516175
body_pct,0.23633267775481184
current_range_pct,0.08648469393553466
upper_wick_pct,0.08407988668198722
lower_wick_pct,0.08170715396069746
h4_close_vs_sma20,0.028191024721587358
gap_pct,0.028053795813908884
gap_size_abs,0.024167428555655724
prev_day_range_pct,0.023781299071907232
swept_prev_low,0.023565542874651124
volume_ratio_5d,0.022180787334961805
h1_eod_volume_ratio,0.018234111769087385
swept_prev_high,0.017826474790047495



================================================
FILE: reports/META_rf_feature_importances.csv
================================================
,0
close_vs_range,0.2923570593544803
body_pct,0.26624292636347086
upper_wick_pct,0.0969208619528452
lower_wick_pct,0.0863499409232801
current_range_pct,0.06707305922830781
swept_prev_low,0.03782056690264039
swept_prev_high,0.030247567198705933
volume_ratio_5d,0.025777268449602063
gap_pct,0.023658782851711465
gap_size_abs,0.01987702914411419
h4_close_vs_sma20,0.018954182670160396
h1_eod_volume_ratio,0.01751899655099915
prev_day_range_pct,0.017201758409682178



================================================
FILE: reports/MSFT_rf_feature_importances.csv
================================================
,0
close_vs_range,0.29626346769843237
body_pct,0.2435288969975542
lower_wick_pct,0.09174666735488504
current_range_pct,0.08522678067232266
upper_wick_pct,0.07898937604714755
swept_prev_low,0.0485393648165831
gap_pct,0.029781825775120906
gap_size_abs,0.027218007715296696
h4_close_vs_sma20,0.026410360368580705
volume_ratio_5d,0.02276345386084945
swept_prev_high,0.01903846043142313
prev_day_range_pct,0.01725303449946871
h1_eod_volume_ratio,0.013240303762335563



================================================
FILE: reports/NVDA_rf_feature_importances.csv
================================================
,0
body_pct,0.29174751335239546
close_vs_range,0.28104014130294136
lower_wick_pct,0.09368930156324458
upper_wick_pct,0.0803463850334476
current_range_pct,0.061166284350087356
volume_ratio_5d,0.03854970894370932
swept_prev_low,0.03360675311446312
prev_day_range_pct,0.028017167584981996
gap_pct,0.022803188773203444
h1_eod_volume_ratio,0.02113996815405715
gap_size_abs,0.02008247213925328
h4_close_vs_sma20,0.019691008548296
swept_prev_high,0.008120107139919374



================================================
FILE: reports/QQQ_rf_feature_importances.csv
================================================
,0
close_vs_range,0.29197103478902486
body_pct,0.23874246317110423
current_range_pct,0.11724761607473078
lower_wick_pct,0.08272207431760238
swept_prev_low,0.0594969510417311
upper_wick_pct,0.058651596423098896
volume_ratio_5d,0.03018298490677117
h4_close_vs_sma20,0.02497101007257103
gap_pct,0.022872961844119667
prev_day_range_pct,0.02041879048052669
swept_prev_high,0.018966537051306444
gap_size_abs,0.01713307529362327
h1_eod_volume_ratio,0.0166229045337895



================================================
FILE: reports/SPY_rf_feature_importances.csv
================================================
,0
close_vs_range,0.3038627140419282
current_range_pct,0.1870991931033024
body_pct,0.16583037062446773
upper_wick_pct,0.07470919557458916
lower_wick_pct,0.05266731727309601
swept_prev_low,0.05087759749719627
volume_ratio_5d,0.031103401989721106
prev_day_range_pct,0.03047976282587159
h4_close_vs_sma20,0.024691951805304323
gap_size_abs,0.02340343574393909
gap_pct,0.019231684388530283
swept_prev_high,0.018859895390362084
h1_eod_volume_ratio,0.017183479741691616



================================================
FILE: reports/TSLA_rf_feature_importances.csv
================================================
,0
close_vs_range,0.28859172332770033
body_pct,0.2849662985544357
lower_wick_pct,0.10996018923917505
upper_wick_pct,0.07316370688335824
current_range_pct,0.04670348205099385
swept_prev_low,0.04462959373222619
volume_ratio_5d,0.024506627165910184
h4_close_vs_sma20,0.023667017363231846
gap_pct,0.02270877850110969
h1_eod_volume_ratio,0.022623445695060394
gap_size_abs,0.0205198164738758
swept_prev_high,0.020299593474729174
prev_day_range_pct,0.017659727538193653



================================================
FILE: src/init.py
================================================
[Empty file]


================================================
FILE: src/data_processing/news_scraper.py
================================================
import requests
from bs4 import BeautifulSoup
import os
import sys
from datetime import datetime

# Add project root to path to allow imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# Import the LLM client to use for summarization
from src.api_clients.llm_client import get_ai_analysis 

# Define the target URL and headers to mimic a browser
NEWS_URL = "https://finviz.com/news.ashx"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def fetch_news_headlines():
    """
    Fetches the latest financial news headlines from Finviz.
    Returns a list of headlines.
    """
    print("  Fetching latest financial news headlines...")
    try:
        response = requests.get(NEWS_URL, headers=HEADERS)
        response.raise_for_status()  # Raise an exception for bad status codes

        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Finviz news headlines are in 'a' tags with class 'nn-tab-link'
        news_table = soup.find('table', class_='news')
        if not news_table:
            print("  Warning: Could not find the news table on Finviz.")
            return []
            
        headlines = [a.text for a in news_table.find_all('a', class_='nn-tab-link')]
        
        # We only need the most recent headlines for pre-market sentiment
        recent_headlines = headlines[:20] # Get the top 20
        print(f"  Successfully fetched {len(recent_headlines)} recent headlines.")
        return recent_headlines

    except requests.exceptions.RequestException as e:
        print(f"  Error fetching news: {e}")
        return []

def summarize_news_with_llm(headlines: list):
    """
    Uses an LLM to summarize a list of headlines into a single paragraph
    describing the overall market sentiment.
    """
    if not headlines:
        return "No news headlines were found to summarize."

    print("  Sending headlines to LLM for sentiment summarization...")
    
    # Format the headlines into a single string for the prompt
    headlines_text = "\n".join(f"- {h}" for h in headlines)
    
    # This is the prompt we designed earlier
    prompt_content = (
        "You are a financial news analyst. Your task is to read the following headlines "
        "and synthesize them into a single, concise paragraph (max 3 sentences) that "
        "summarizes the overall market sentiment for the upcoming trading session. "
        "Focus on macroeconomic news and sentiment for the major indices (S&P 500, Nasdaq).\n\n"
        f"**News Headlines:**\n{headlines_text}\n\n"
        "**Your Summary:**"
    )
    
    try:
        # We'll use a generic system prompt for this simple task
        system_prompt = "You are a helpful assistant that provides concise summaries."
        summary = get_ai_analysis(system_prompt, prompt_content) # Assumes llm_client returns just the text content
        
        print("  LLM summary received.")
        return summary

    except Exception as e:
        print(f"  Error getting LLM summary: {e}")
        return "Could not generate news summary due to an error."

def get_daily_news_summary():
    """
    The main function for this module. Fetches and summarizes daily news.
    """
    print("\n--- Starting Daily News & Sentiment Analysis ---")
    headlines = fetch_news_headlines()
    summary = summarize_news_with_llm(headlines)
    print("--- News Analysis Complete ---")
    return summary

if __name__ == '__main__':
    # This allows you to test the script directly
    summary = get_daily_news_summary()
    print("\n--- TEST SUMMARY ---")
    print(summary)



================================================
FILE: training/train_daily_bias_models.py
================================================
import os
import sys
import pandas as pd
import numpy as np
import pickle
from datetime import datetime, timedelta, timezone
import warnings
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score
from dotenv import load_dotenv
from pathlib import Path

# ---- NEW: modern Alpaca SDK imports ----
from alpaca.trading.client import TradingClient
from alpaca.data.historical import StockHistoricalDataClient
from alpaca.data.requests import StockBarsRequest
from alpaca.data.timeframe import TimeFrame, TimeFrameUnit
from alpaca.data.enums import DataFeed

# Add the project root to the Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# --- Import watchlist from central config ---
from config.settings import WATCHLIST

# ---- Load .env (project root first) ----
try:
    dotenv_path = Path(__file__).resolve().parents[1] / '.env'
    print(f"Attempting to load .env from: {dotenv_path}")
    if dotenv_path.exists():
        load_dotenv(dotenv_path=dotenv_path)
        print(".env file loaded successfully.")
    else:
        print("Warning: .env file not found at the expected project root location.")
        load_dotenv()  # Fallback
except Exception as e:
    print(f"Error loading .env file: {e}")
    load_dotenv()

warnings.filterwarnings('ignore')

# ---- Initialize Alpaca clients (modern) ----
trading = None
data = None
try:
    # Keep your existing env var names
    api_key = os.getenv("ALPACA_API_KEY") or os.getenv("ALPACA_API_KEY_ID")
    api_secret = os.getenv("ALPACA_SECRET_KEY") or os.getenv("ALPACA_API_SECRET_KEY")
    paper = (os.getenv("ALPACA_PAPER_TRADING", "true").lower() == "true")

    if not api_key or not api_secret:
        raise ValueError("Alpaca API keys not found in .env file.")

    # Trading client (toggle paper vs live)
    trading = TradingClient(api_key, api_secret, paper=paper)

    # Data client (keys optional for some feeds; we pass keys for full access)
    data = StockHistoricalDataClient(api_key, api_secret)

    # Choose feed via env, default to SIP if available; otherwise IEX
    # ALPACA_DATA_FEED=sip|iex
    feed_env = (os.getenv("ALPACA_DATA_FEED") or "iex").lower()
    FEED = DataFeed.SIP if feed_env == "sip" else DataFeed.IEX

    print("✅ Alpaca clients initialized (alpaca-py). Data feed:", FEED.value)
except Exception as e:
    print(f"❌ Error initializing Alpaca clients: {e}")
    trading, data = None, None

# --- Helper: safe divide ---
def _safe_divide(numerator, denominator, default=0):
    return np.where(denominator != 0, numerator / denominator, default)

# --- Helper: normalize bars DF to a simple DatetimeIndex for a single symbol ---
def _as_single_symbol_df(bars_df: pd.DataFrame, symbol: str) -> pd.DataFrame:
    """
    alpaca-py returns a MultiIndex (symbol, timestamp). This flattens to a DatetimeIndex for the given symbol.
    """
    if bars_df is None or bars_df.empty:
        return pd.DataFrame()
    if bars_df.index.nlevels == 2:
        try:
            df = bars_df.xs(symbol, level=0).copy()
        except KeyError:
            return pd.DataFrame()
    else:
        df = bars_df.copy()
    # Ensure datetime is tz-aware (usually UTC from alpaca-py)
    if df.index.tz is None:
        df.index = df.index.tz_localize(timezone.utc)
    return df.sort_index()

# --- NEW: Upgraded Data Fetching for Multi-Timeframe Analysis ---
def download_data(symbol):
    """Downloads daily, 4-hour, and 1-hour historical data for a given symbol from Alpaca (alpaca-py)."""
    if data is None:
        print("  Alpaca data client not available. Skipping data download.")
        return None, None, None

    print(f"  Downloading multi-timeframe data for {symbol}...")
    try:
        # Use timezone-aware UTC datetimes for requests
        end_dt = datetime.now(timezone.utc)
        # 5y daily, 2y 4H, ~700d 1H (roughly 2y)
        start_1d = end_dt - timedelta(days=5 * 365)
        start_4h = end_dt - timedelta(days=2 * 365)
        start_1h = end_dt - timedelta(days=700)

        # Daily bars
        req_1d = StockBarsRequest(
            symbol_or_symbols=[symbol],
            timeframe=TimeFrame.Day,
            start=start_1d,
            end=end_dt,
            feed=FEED,
            adjustment='raw',
            limit=None,
        )
        bars_1d = data.get_stock_bars(req_1d).df
        df_1d = _as_single_symbol_df(bars_1d, symbol)

        # 4-Hour bars (TimeFrame with unit/multiplier)
        req_4h = StockBarsRequest(
            symbol_or_symbols=[symbol],
            timeframe=TimeFrame(4, TimeFrameUnit.Hour),
            start=start_4h,
            end=end_dt,
            feed=FEED,
            adjustment='raw',
            limit=None,
        )
        bars_4h = data.get_stock_bars(req_4h).df
        df_4h = _as_single_symbol_df(bars_4h, symbol)

        # 1-Hour bars
        req_1h = StockBarsRequest(
            symbol_or_symbols=[symbol],
            timeframe=TimeFrame.Hour,
            start=start_1h,
            end=end_dt,
            feed=FEED,
            adjustment='raw',
            limit=None,
        )
        bars_1h = data.get_stock_bars(req_1h).df
        df_1h = _as_single_symbol_df(bars_1h, symbol)

        if df_1d.empty or df_4h.empty or df_1h.empty:
            print(f"  Warning: Could not fetch complete data for {symbol}.")
            return None, None, None

        print(f"  Downloaded: {len(df_1d)} days, {len(df_4h)} 4H bars, {len(df_1h)} 1H bars.")
        return df_1d, df_4h, df_1h

    except Exception as e:
        if "is not a valid symbol" in str(e).lower():
            print(f"  Error: '{symbol}' is not a valid stock/ETF symbol for Alpaca. Skipping.")
        else:
            print(f"  Error downloading data for {symbol}: {e}")
        return None, None, None

# --- Merge timeframes and create HTF features ---
def merge_and_enrich_data(df_1d, df_4h, df_1h):
    """Merges H4 and H1 data into the daily dataframe to create richer features."""
    print("  Merging timeframes and creating HTF features...")
    df_1d = df_1d.copy()

    # Pre-calc HTF indicators on their native frames
    df_4h = df_4h.copy()
    df_1h = df_1h.copy()
    df_4h['sma_20'] = df_4h['close'].rolling(20).mean()
    df_1h['volume_sma_10'] = df_1h['volume'].rolling(10).mean()

    # Create target columns on daily frame
    df_1d['h4_close_vs_sma20'] = np.nan
    df_1d['h1_eod_volume_ratio'] = np.nan

    # Timezone handling: Alpaca returns UTC. Convert for the “3 PM ET” logic.
    h1_eastern = df_1h.tz_convert("US/Eastern")

    # Iterate through each day and map HTF context
    for date, _ in df_1d.iterrows():
        # 'date' is a UTC timestamp for the end of day bar
        # Use prior day (US/Eastern) to find the 3pm ET hour bar
        prev_day_utc = (pd.Timestamp(date).tz_convert("US/Eastern") - pd.Timedelta(days=1)).date()

        # Last 4H bar strictly before the daily bar time
        last_4h_bar = df_4h[df_4h.index < date].last("1D")
        if not last_4h_bar.empty:
            last_close = last_4h_bar.iloc[-1]['close']
            last_sma = last_4h_bar.iloc[-1]['sma_20']
            df_1d.loc[date, 'h4_close_vs_sma20'] = float(_safe_divide(last_close - last_sma, last_sma) * 100)

        # The 3 PM ET 1H bar of the previous calendar day in ET
        mask_prev_day_3pm = (h1_eastern.index.date == prev_day_utc) & (h1_eastern.index.hour == 15)
        last_1h_bars = h1_eastern[mask_prev_day_3pm]
        if not last_1h_bars.empty:
            last_volume = last_1h_bars.iloc[-1]['volume']
            avg_volume = last_1h_bars.iloc[-1]['volume_sma_10']
            df_1d.loc[date, 'h1_eod_volume_ratio'] = float(_safe_divide(last_volume, avg_volume, default=np.nan))

    return df_1d

# --- Daily feature engineering ---
def engineer_features(df):
    df = df.copy()
    # Ensure expected lower-case columns
    df.columns = [x.lower() for x in df.columns]

    # Previous day reference
    df['prev_close'] = df['close'].shift(1)
    df['prev_high'] = df['high'].shift(1)
    df['prev_low'] = df['low'].shift(1)

    # Basic features
    df['gap_pct'] = _safe_divide((df['open'] - df['prev_close']), df['prev_close']) * 100
    df['gap_size_abs'] = abs(df['gap_pct'])

    # ICT-ish / technical features
    df['prev_day_range_pct'] = _safe_divide((df['prev_high'] - df['prev_low']), df['prev_close']) * 100
    df['current_range_pct'] = _safe_divide((df['high'] - df['low']), df['open']) * 100
    vol_5d = df['volume'].rolling(5, min_periods=1).mean()
    df['volume_ratio_5d'] = _safe_divide(df['volume'], vol_5d, 1.0)
    df['swept_prev_high'] = (df['high'] > df['prev_high']).astype(int)
    df['swept_prev_low']  = (df['low']  < df['prev_low']).astype(int)
    df['body_size'] = (df['close'] - df['open']).abs()
    df['total_range'] = df['high'] - df['low']
    df['body_pct'] = _safe_divide(df['body_size'], df['total_range']) * 100
    df['upper_wick_pct'] = _safe_divide(df['high'] - df[['open', 'close']].max(axis=1), df['total_range']) * 100
    df['lower_wick_pct'] = _safe_divide(df[['open', 'close']].min(axis=1) - df['low'], df['total_range']) * 100
    range_nonzero = np.maximum(df['high'] - df['low'], 1e-10)
    df['close_vs_range'] = _safe_divide((df['close'] - df['low']), range_nonzero)

    # Clean
    df = df.replace([np.inf, -np.inf], np.nan).dropna()
    return df

# --- Labeling ---
def define_bias_label(df, threshold=0.5):
    daily_move_pct = _safe_divide(df['close'] - df['open'], df['open']) * 100
    conditions = [daily_move_pct > threshold, daily_move_pct < -threshold]
    choices = ['bullish', 'bearish']
    df = df.copy()
    df['bias_label'] = np.select(conditions, choices, default='choppy')
    return df

# --- Train & save ---
def train_and_save_model(symbol, data_with_features):
    feature_names = [
        'gap_pct', 'gap_size_abs', 'prev_day_range_pct', 'current_range_pct',
        'volume_ratio_5d', 'swept_prev_high', 'swept_prev_low', 'body_pct',
        'upper_wick_pct', 'lower_wick_pct', 'close_vs_range',
        'h4_close_vs_sma20', 'h1_eod_volume_ratio'
    ]
    for f in feature_names:
        if f not in data_with_features.columns:
            data_with_features[f] = 0

    X = data_with_features[feature_names]
    y = data_with_features['bias_label']

    le = LabelEncoder()
    y_enc = le.fit_transform(y)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y_enc, test_size=0.2, random_state=42, stratify=y_enc
    )
    model = RandomForestClassifier(
        n_estimators=200, max_depth=10, min_samples_split=5,
        random_state=42, class_weight='balanced'
    )
    print("  Training model on enriched data...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    # --- Built-in (Gini) feature importances
    importances = model.feature_importances_
    feat_ranking = (
        pd.Series(importances, index=feature_names)
        .sort_values(ascending=False)
    )

    print("\nTop feature importances (model intrinsic):")
    print(feat_ranking.to_string(float_format=lambda v: f"{v:.4f}"))

    # Save to disk per symbol
    reports_dir = os.path.join(os.path.dirname(__file__), '..', 'reports')
    os.makedirs(reports_dir, exist_ok=True)
    csv_path = os.path.join(reports_dir, f"{symbol}_rf_feature_importances.csv")
    feat_ranking.to_csv(csv_path)

    # PNG (top 20)
    import matplotlib
    matplotlib.use("Agg")  # safe for headless envs
    import matplotlib.pyplot as plt

    plt.figure()
    feat_ranking.head(20).plot(kind="bar")
    plt.title(f"{symbol} – RF feature importances (top 20)")
    plt.ylabel("Importance")
    plt.tight_layout()
    png_path = os.path.join(reports_dir, f"{symbol}_rf_feature_importances_top20.png")
    plt.savefig(png_path)
    plt.close()

    print(f"  Model for {symbol} trained with Test Accuracy: {acc:.2%}")

    

    models_dir = os.path.join(os.path.dirname(__file__), '..', 'models')
    os.makedirs(models_dir, exist_ok=True)
    with open(os.path.join(models_dir, f'{symbol}_daily_bias.pkl'), 'wb') as f:
        pickle.dump(model, f)
    with open(os.path.join(models_dir, f'{symbol}_label_encoder.pkl'), 'wb') as f:
        pickle.dump(le, f)
    print(f"  Models for {symbol} saved successfully to '{models_dir}'.")

# --- Pipeline ---
def run_training_pipeline():
    print("="*60)
    print("Starting Multi-Timeframe Daily Bias Model Training Pipeline")
    print(f"Symbols to process: {', '.join(WATCHLIST)}")
    print("="*60)

    for symbol in WATCHLIST:
        print(f"\n--- Processing Symbol: {symbol} ---")
        df_1d, df_4h, df_1h = download_data(symbol)
        if df_1d is None:
            continue

        enriched = merge_and_enrich_data(df_1d, df_4h, df_1h)
        feats = engineer_features(enriched)
        labeled = define_bias_label(feats)

        if not labeled.empty:
            train_and_save_model(symbol, labeled)
        else:
            print("  Not enough data to train model after feature engineering.")

    print("\n--- Pipeline Complete: All models have been trained and saved. ---")

if __name__ == "__main__":
    if data is not None:
        run_training_pipeline()
    else:
        print("\nExiting: Alpaca data client could not be initialized.")
        sys.exit(1)



================================================
FILE: training/notebooks/feature_engineering.ipynb
================================================
# Jupyter notebook converted to Python script.

"""
# Feature Engineering Workbench

**Objective:** This notebook is a laboratory for exploring, visualizing, and testing new features for the Daily Bias prediction models. 

**Workflow:**
1.  Load historical multi-timeframe data for a single symbol.
2.  Apply the existing feature engineering logic (from `train_daily_bias_models.py`).
3.  Visualize the relationships between features and the target bias.
4.  Experiment with creating new, experimental features.
5.  Once a new feature is proven to be valuable, its logic should be copied into the main `engineer_features` function in the production training script.
"""

"""
## 1. Setup and Data Loading
"""

import os
import sys
import pandas as pd
import numpy as np
import pickle
from datetime import datetime, timedelta
import warnings
import alpaca_trade_api as tradeapi
from dotenv import load_dotenv
import matplotlib.pyplot as plt
import seaborn as sns

# Add project root to path to allow imports from config
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath('__file__')))))
from config.settings import WATCHLIST

warnings.filterwarnings('ignore')
sns.set_style('whitegrid')

# --- Load API Keys ---
load_dotenv(dotenv_path='../../.env')
api_key = os.getenv('ALPACA_API_KEY')
secret_key = os.getenv('ALPACA_SECRET_KEY')

api = tradeapi.REST(
    key_id=api_key,
    secret_key=secret_key,
    base_url='https://paper-api.alpaca.markets'
)

print("Libraries loaded and Alpaca API client initialized.")

# --- Single Symbol Data Loading ---
SYMBOL_TO_ANALYZE = 'SPY' # Change this to analyze different symbols

print(f"Fetching data for {SYMBOL_TO_ANALYZE}...")

end_date = datetime.now().isoformat()
start_1d = (datetime.now() - timedelta(days=5*365)).isoformat()

df_daily = api.get_bars(SYMBOL_TO_ANALYZE, '1Day', start=start_1d, end=end_date).df

print(f"Downloaded {len(df_daily)} daily bars.")
df_daily.head()

"""
## 2. Feature Engineering Logic

This section contains the exact same feature engineering functions from the production `train_daily_bias_models.py` script. This ensures our research environment matches our production environment.
"""

def _safe_divide(numerator, denominator, default=0):
    return np.where(denominator != 0, numerator / denominator, default)

def engineer_features(df):
    df = df.copy()
    df.columns = [x.lower() for x in df.columns]

    df['prev_close'] = df['close'].shift(1)
    df['prev_high'] = df['high'].shift(1)
    df['prev_low'] = df['low'].shift(1)

    # Basic & ICT Features
    df['gap_pct'] = _safe_divide((df['open'] - df['prev_close']), df['prev_close']) * 100
    df['gap_size_abs'] = abs(df['gap_pct'])
    df['prev_day_range_pct'] = _safe_divide((df['prev_high'] - df['prev_low']), df['prev_close']) * 100
    df['current_range_pct'] = _safe_divide((df['high'] - df['low']), df['open']) * 100
    df['volume_ratio_5d'] = _safe_divide(df['volume'], df['volume'].rolling(5, min_periods=1).mean(), 1.0)
    df['swept_prev_high'] = np.where(df['high'] > df['prev_high'], 1, 0)
    df['swept_prev_low'] = np.where(df['low'] < df['prev_low'], 1, 0)
    df['body_size'] = abs(df['close'] - df['open'])
    df['total_range'] = df['high'] - df['low']
    df['body_pct'] = _safe_divide(df['body_size'], df['total_range']) * 100
    df['upper_wick_pct'] = _safe_divide(df['high'] - df[['open', 'close']].max(axis=1), df['total_range']) * 100
    df['lower_wick_pct'] = _safe_divide(df[['open', 'close']].min(axis=1) - df['low'], df['total_range']) * 100
    range_nonzero = np.maximum(df['high'] - df['low'], 1e-10)
    df['close_vs_range'] = _safe_divide((df['close'] - df['low']), range_nonzero)
    
    df = df.replace([np.inf, -np.inf], np.nan).dropna()
    return df

def define_bias_label(df, threshold=0.5):
    daily_move_pct = _safe_divide(df['close'] - df['open'], df['open']) * 100
    conditions = [daily_move_pct > threshold, daily_move_pct < -threshold]
    choices = ['bullish', 'bearish']
    df['bias_label'] = np.select(conditions, choices, default='choppy')
    return df

# --- Run the feature engineering pipeline ---
df_features = engineer_features(df_daily)
df_labeled = define_bias_label(df_features)

print("Feature engineering complete.")
df_labeled[['gap_pct', 'volume_ratio_5d', 'swept_prev_high', 'body_pct', 'bias_label']].head()

"""
## 3. Visualization and Analysis

Let's visualize some of the features we created to understand their relationship with the daily bias.
"""

# Visualize the distribution of the target variable
plt.figure(figsize=(8, 5))
sns.countplot(data=df_labeled, x='bias_label', palette='viridis')
plt.title(f'Distribution of Daily Bias Labels for {SYMBOL_TO_ANALYZE}', fontsize=16)
plt.ylabel('Count')
plt.xlabel('Bias Label')
plt.show()

# Visualize the relationship between Gap Percentage and Bias
plt.figure(figsize=(10, 6))
sns.boxplot(data=df_labeled, x='bias_label', y='gap_pct', palette='coolwarm')
plt.title('Gap Percentage vs. Daily Bias', fontsize=16)
plt.axhline(0, color='grey', linestyle='--')
plt.ylabel('Gap %')
plt.xlabel('Bias Label')
plt.show()

# --- Correlation Heatmap ---
feature_names = [
    'gap_pct', 'gap_size_abs', 'prev_day_range_pct', 'current_range_pct',
    'volume_ratio_5d', 'swept_prev_high', 'swept_prev_low', 'body_pct',
    'upper_wick_pct', 'lower_wick_pct', 'close_vs_range'
]

corr_matrix = df_labeled[feature_names].corr()

plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, annot=True, cmap='viridis', fmt='.2f')
plt.title('Feature Correlation Matrix', fontsize=16)
plt.show()

"""
## 4. Experimental Zone: Creating a New Feature

This is where you can test ideas for new features. Let's try creating a 'consecutive_day_direction' feature.
"""

def add_experimental_feature(df):
    df_exp = df.copy()
    
    # Calculate the direction of the previous day's move
    daily_move = df_exp['close'] - df_exp['open']
    direction = np.sign(daily_move).shift(1).fillna(0)
    
    # Calculate consecutive days in the same direction
    consecutive_days = direction.groupby((direction != direction.shift()).cumsum()).cumcount() + 1
    df_exp['consecutive_day_direction'] = consecutive_days * direction
    
    return df_exp

df_experimental = add_experimental_feature(df_labeled)

# Visualize the new feature
plt.figure(figsize=(10, 6))
sns.boxplot(data=df_experimental, x='bias_label', y='consecutive_day_direction', palette='plasma')
plt.title('Experimental Feature: Consecutive Day Direction vs. Bias', fontsize=16)
plt.ylabel('Consecutive Days (Positive=Up, Negative=Down)')
plt.xlabel('Bias Label')
plt.show()

df_experimental[['consecutive_day_direction', 'bias_label']].head(10)


